{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "from typing import Any, Annotated, TypedDict, Literal\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool, StructuredTool\n",
    "\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "import io\n",
    "from IPython.display import display, Image as IPImage, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_camera():\n",
    "\n",
    "    # Initialize the pipeline\n",
    "    pipe = rs.pipeline()\n",
    "    config = rs.config()\n",
    "    \n",
    "    # Enable color stream \n",
    "    config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "\n",
    "    # Start Streaming \n",
    "    pipe.start(config)\n",
    "    return pipe\n",
    "\n",
    "def capture_frame(pipe):\n",
    "\n",
    "    # Skip the initial frames to stabilize the camera\n",
    "    for _ in range(20):\n",
    "        pipe.wait_for_frames()\n",
    "\n",
    "    frames = pipe.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "\n",
    "    if not color_frame:\n",
    "        return None\n",
    "    \n",
    "    # Convert images to numpy arrays\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    # Convert from BGR to RGB\n",
    "    color_image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "    return color_image_rgb\n",
    "\n",
    "def prepare_image_for_api(image):\n",
    "    # Convert numpy array to PIL Image\n",
    "    pil_image = Image.fromarray(image)\n",
    "\n",
    "    # Create a byte stream \n",
    "    byte_stream = io.BytesIO()\n",
    "\n",
    "    # Save the image to the byte stream in JPEG format \n",
    "    pil_image.save(byte_stream, format='JPEG')\n",
    "\n",
    "    # Get the byte value and encode to base64\n",
    "    img_bytes = byte_stream.getvalue()\n",
    "    base64_image = base64.b64encode(img_bytes).decode('utf-8')\n",
    "\n",
    "    return base64_image, byte_stream.getvalue()\n",
    "\n",
    "\n",
    "def analyze_image_with_openai(base64_image, prompt):\n",
    "    client = OpenAI()\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "             messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing image: {str(e)}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_and_analyze_frame(prompt: str):\n",
    "    \"\"\"\n",
    "    Captures a frame from the camera, prepares it for API analysis, and analyzes it using OpenAI.\n",
    "\n",
    "    Returns:\n",
    "        result: The analysis result from OpenAI or None if the frame couldn't be captured.\n",
    "    \"\"\"\n",
    "    pipe = initialize_camera()\n",
    "\n",
    "    try:\n",
    "        # Capture a frame from the camera\n",
    "        frame = capture_frame(pipe)\n",
    "\n",
    "        if frame is not None:\n",
    "            # Prepare the frame for API analysis\n",
    "            base64_image, jpeg_bytes = prepare_image_for_api(frame)\n",
    "\n",
    "            # Analyze the prepared image using OpenAI\n",
    "            result = analyze_image_with_openai(base64_image, prompt)\n",
    "\n",
    "            return result\n",
    "        else:\n",
    "            print(\"No frame captured.\")\n",
    "            return None\n",
    "    \n",
    "    finally:\n",
    "        # Ensure the camera pipeline is stopped\n",
    "        pipe.stop()\n",
    "\n",
    "\n",
    "def list_usb_devices() -> list:\n",
    "    \"\"\"\n",
    "    Retrieves a list of all USB devices connected to the system using the lsusb command.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of strings representing USB devices with their details\n",
    "    \"\"\"\n",
    "    devices = []\n",
    "    result = subprocess.run(['lsusb'], capture_output=True, text=True, check=True)\n",
    "    for line in result.stdout.strip().split('\\n'):\n",
    "        devices.append(line)\n",
    "    return devices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the default state same as \"MessagesState\" TypedDict but allows us accessibility to custom keys\n",
    "class GraphsState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages] \n",
    "    # Custom keys for additional data can be added here such as - conversation_id: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [capture_and_analyze_frame, list_usb_devices]\n",
    "\n",
    "# Initialize LLM \n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are a helpful assistant with two primary capabilities:\n",
    "\n",
    "   1. Vision Analysis\n",
    "        You can analyze images captured by a camera and provide insightful, detailed observations in response to user queries.\n",
    "        When a vision-related question is asked, carefully examine the provided image and describe relevant features, objects, contexts, or any other notable elements.\n",
    "\n",
    "    2. Device Management\n",
    "        You can identify and manage devices connected to the system, offering precise information and instructions based on their specifications or status.\n",
    "        When a device-related question is asked, reference the current list of connected devices (or relevant data) to provide accurate, in-depth answers, including guidance on setup, configuration, troubleshooting, or usage.\n",
    "\n",
    "Instructions\n",
    "\n",
    "    For vision-related queries, provide clear, comprehensive image analyses.\n",
    "    For device-related queries, deliver precise, actionable details that address the userâ€™s request.\n",
    "    Always strive to give concise, user-friendly, and contextually appropriate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Message\n",
    "sys_msg = SystemMessage(\n",
    "    content=(\"\"\" You are a helpful assistant with two main capabilities:\n",
    "    1. Vision Analysis: You can analyze images captured by the camera and provide detailed descriptions based on the user's query.\n",
    "    2. Device Management: You can identify and manage devices connected to the system.\n",
    "    \n",
    "    For vision-related queries, you'll analyze images and provide detailed observations.\n",
    "    For device-related queries, you'll provide precise and detailed responses based on the input lists\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Node \n",
    "def assistant(state):\n",
    "    \"\"\"Process messages in the state and return LLM response.\"\"\"\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "\n",
    "# Graph setup \n",
    "graph = StateGraph(GraphsState)\n",
    "graph.add_node(\"assistant\", assistant)\n",
    "graph.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges\n",
    "graph.add_edge(START, \"assistant\")\n",
    "graph.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "# Compile graph\n",
    "graph_runnable = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What are the objects you see in the image\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  capture_and_analyze_frame (call_koLTWaQHg3LOH1nefiIHWjAA)\n",
      " Call ID: call_koLTWaQHg3LOH1nefiIHWjAA\n",
      "  Args:\n",
      "    prompt: Identify the objects present in the scene from the captured image.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: capture_and_analyze_frame\n",
      "\n",
      "The image shows a few objects and elements:\n",
      "\n",
      "1. A person wearing a jacket and a gray shirt.\n",
      "2. A television or monitor displaying an image of a person, located to the left of the scene.\n",
      "3. A small green and black object on the table, possibly a computer mouse or a remote.\n",
      "4. A table or desk surface.\n",
      "5. A window with blinds in the background. \n",
      "\n",
      "The setting appears to be indoors, in a room with a desk or table.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The image contains the following objects and elements:\n",
      "\n",
      "1. A person wearing a jacket and a gray shirt.\n",
      "2. A television or monitor displaying an image of a person, located to the left of the scene.\n",
      "3. A small green and black object on the table, possibly a computer mouse or remote.\n",
      "4. A table or desk surface.\n",
      "5. A window with blinds in the background.\n",
      "\n",
      "The setting seems to be indoors, in a room with a desk or table.\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What are the objects you see in the image\")]\n",
    "messages = graph_runnable.invoke({\"messages\": messages})\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "is there a camera connected\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  list_usb_devices (call_r5bDSXhnJlNhBldhgqvW25AM)\n",
      " Call ID: call_r5bDSXhnJlNhBldhgqvW25AM\n",
      "  Args:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: list_usb_devices\n",
      "\n",
      "[\"Bus 002 Device 010: ID 8086:0b07 Intel Corp. RealSense D435\", \"Bus 002 Device 002: ID 0bda:0420 Realtek Semiconductor Corp. 4-Port USB 3.0 Hub\", \"Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\", \"Bus 001 Device 005: ID 062a:38df MosArt Semiconductor Corp. TIETI UltraSlim KB\", \"Bus 001 Device 004: ID 1bcf:08a0 Sunplus Innovation Technology Inc. Gaming mouse [Philips SPK9304]\", \"Bus 001 Device 003: ID 0bda:5420 Realtek Semiconductor Corp. 4-Port USB 2.0 Hub\", \"Bus 001 Device 002: ID 13d3:3549 IMC Networks Bluetooth Radio\", \"Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\"]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, there is a camera connected. The connected camera is an Intel RealSense D435, identified by the device ID `8086:0b07`.\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"is there a camera connected\")]\n",
    "messages = graph_runnable.invoke({\"messages\": messages})\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
